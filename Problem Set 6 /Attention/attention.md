# Attention Model: Predicting Masked Words in Text Sequences

This project builds an AI system to predict a masked word in a given text sequence using a Masked Language Model (MLM). The model is based on **BERT** (Bidirectional Encoder Representations from Transformers), a transformer-based language model developed by Google. BERT uses an attention mechanism to understand the relationship between words in a sequence and predict the masked word.

## Project Overview

In this project, we will:
- Use BERT to predict a masked word in a sentence.
- Visualize the attention mechanism of BERT, which shows how the model processes the sequence and learns to focus on relevant parts of the input text to make predictions.
- Analyze the attention heads to gain insight into how the model interprets language.

## Requirements

- **Python Version**: 3.12 (or earlier versions)
- **Dependencies**: All required dependencies can be installed via the provided `requirements.txt` file.

## Project Setup

### 1. Download Code
Unzip the provided distribution code from the CS50 Attention Zip.

### 2. Install Dependencies
Inside the project directory, install the required dependencies by using the `requirements.txt` file.

### 3. Run the Program
Run the program to predict the masked words in text sequences.

### Sample Output

**Input Text:**
- "We turned down a narrow lane and passed through a small [MASK]."
- "Then I picked up a [MASK] from the table."

**Output:**
- The model will predict several possible words to replace the [MASK] token, such as "field", "book", or "bottle".

## Getting Help

- **Ed**: Ask questions via the Ed platform.
- **CS50 Communities**: Engage with any of CS50’s community channels for assistance.

## Background

BERT is a transformer-based model that leverages attention mechanisms to predict missing words (masked tokens) in a sequence of text. The attention heads in BERT help the model understand the relationships between words in a sequence, even when the words are far apart.

This project aims to:
1. **Tokenize** the input text to split it into smaller tokens, using the [MASK] token to represent the word to be predicted.
2. **Predict** the masked word using the pre-trained BERT model (TFBertForMaskedLM).
3. **Visualize** the attention mechanisms to understand which parts of the text the model focuses on to make its predictions.

## Example Attention Analysis

Here’s an example of how attention heads in BERT focus on different parts of a sequence:

- **Attention Layer 3, Head 10**: This attention head focuses on the next word in the sequence, helping the model understand the relationship between tokens like "picked" and the masked word (e.g., "book", "bottle", or "plate").
  
- **Attention Layer 4, Head 11**: This attention head focuses on the relationship between adverbs and verbs, like in the sentence "The turtle moved slowly across the [MASK]", where the attention helps the model focus on the verb "moved".

## Tasks to Complete

### 1. Complete the `TODO` sections in the `analysis.md` file
- Describe at least two attention heads, explaining their learned relationships in language. Provide example sentences and analyze the diagrams generated by your model to infer what these attention heads are focusing on.

### 2. Analyze Attention Behavior
- Attention heads can sometimes focus on special tokens like `[CLS]` or `[SEP]`, especially when there is not enough context to focus on other parts of the sentence.
- Attention heads may appear noisy or difficult to interpret at first. It’s important to run multiple test cases and apply your intuition to understand their behavior.

## Contributing

If you would like to contribute to this project, feel free to open an issue or submit a pull request. Your contributions are welcome!

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
